{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc95f30",
   "metadata": {},
   "source": [
    "# Multi-label Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfca0",
   "metadata": {},
   "source": [
    "SemEval2025 Task 11 Track A + C\n",
    "\n",
    "Finetune model to generate code\n",
    "use our data to fine-tune and evaluate\n",
    "Evaluate fine-tuned vs zero-shot vs baseline (generate n times and record error rate)\n",
    "Use a dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28832",
   "metadata": {},
   "source": [
    "## 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d637bf",
   "metadata": {},
   "source": [
    "#### Installing other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41206a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# set a seed value\n",
    "torch.manual_seed(69)\n",
    "\n",
    "import sentencepiece\n",
    "\n",
    "#possibly remove some metrics later\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "#Choose between Bert and XLM Roberta\n",
    "#from transformers import BertTokenizer, BertForSequenceClassification \n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee52cb",
   "metadata": {},
   "source": [
    "## 1B. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "def split_data(data, test_size=0.2, random_state=42):\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return train_data, val_data\n",
    "\n",
    "# Custom Dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.iloc[index][\"text\"]\n",
    "        labels = self.data.iloc[index][[\"Anger\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]].values.astype(float)\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize tokenizer and model\n",
    "def initialize_model_and_tokenizer():\n",
    "    #USE FOR XLM ROBERTA\n",
    "    MODEL_TYPE = 'xlm-roberta-base'\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_TYPE, num_labels = 5\n",
    "    )\n",
    "\n",
    "    #USE FOR DISTILBERT (not currently in use, need to uncomment autotokenizer in 1A too)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        #\"distilbert-base-multilingual-cased\", num_labels=5\n",
    "    #)\n",
    "    #return tokenizer, model\n",
    "\n",
    "    #USE FOR BERT\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "    #model = BertForSequenceClassification.from_pretrained(\n",
    "        #'bert-base-multilingual-cased', \n",
    "        #num_labels = 5,\n",
    "        #output_attentions = False,\n",
    "        #output_hidden_states = False\n",
    "    #)\n",
    "    return tokenizer, model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=3, learning_rate=5e-5):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "\n",
    "            preds = torch.sigmoid(outputs.logits)\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(np.array(true_labels), np.array(pred_labels), average=\"macro\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = \"public_data/train/track_a/eng.csv\"  # Your complete dataset file\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "data = load_data(dataset_file_path)\n",
    "\n",
    "# Split the dataset into training and validation sets (80/20 split)\n",
    "train_data, val_data = split_data(data, test_size=0.2)\n",
    "train_data = train_data.sample(n=100, random_state=42)\n",
    "val_data = val_data.sample(n=20, random_state=42)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer, model = initialize_model_and_tokenizer()\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = EmotionDataset(train_data, tokenizer, max_length)\n",
    "val_dataset = EmotionDataset(val_data, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, val_loader, device, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
